# Домашнее задание: Вероятностные основы модели линейной регрессии


## Задача

В этом домашнем задании мы рассмотрим вывод функции потерь для линейной регрессии в предположении, что целевая переменная генерируется природой из распределения Лапласа, а не из нормального распределения.

## Формулировка задачи

Пусть у нас есть:

- $X = (x_1, \ldots, x_d)$ - вектор признакового описания объекта, где $d$ - количество признаков
- $y$ - целевая переменная
- $X \in \mathbb{R}^d$ - тип признакового описания (вещественные числа)
- $y \in \mathbb{R}$ - тип целевой переменной (вещественное число)
- $J = (X, y)$
## Предположения модели

1) Линейная зависимость между признаками и целевой переменной:

$$f(X, \theta) = \theta^T X$$

   где $\theta$ - параметры модели

2) Целевая переменная $y$ генерируется из распределения Лапласа:

$$y \sim \text{Laplace}(f(X, \theta), b)$$

   Плотность распределения Лапласа:

$$p(y|X, \theta, b) = \frac{1}{2b} \exp\left(-\frac{|y - f(X, \theta)|}{b}\right)$$

   где $b > 0$ - параметр масштаба

3) **i.i.d** - Независимость и одинаковая распределенность наблюдений в выборке 

4) Отсутствие мультиколлинеарности признаков

## Дополнительные переменные

- $\theta = (\theta_0, \theta_1, \ldots, \theta_d)$ - вектор параметров модели
- $b$ - параметр масштаба распределения Лапласа
- $n$ - количество объектов в выборке
- $(X_i, y_i)$ - $i$-й объект выборки, где $i = 1, \ldots, n$

## Правдоподобие
### Теорема Байеса

$$P(\theta|J) = \frac{P(J|\theta)P(\theta)}{P(J)}$$

где:
- $P(\theta)$ - априорное распределение
- $P(\theta|J)$ - апостериорное распределение
- $P(J|\theta)$ - правдоподобие
- $P(J)$ -  распределение данных

Оценка максимального правдоподобия:

$$\hat{\Theta} = \arg\max_{\Theta} P(J|\theta)$$

Правдоподобие для одного объекта:

$$L_i(\theta, b|X_i, y_i) = p(y_i|X_i, \theta, b) = \frac{1}{2b} \exp\left(-\frac{|y_i - f(X_i, \theta)|}{b}\right)$$

Правдоподобие для всей выборки:

$$L(\theta, b|X, y) = \prod\limits_{i=1}^n L_i(\theta, b|X_i, y_i) = \prod\limits_{i=1}^n \frac{1}{2b} \exp\left(-\frac{|y_i - f(X_i, \theta)|}{b}\right)$$

## Подход максимизации правдоподобия

Метод максимального правдоподобия заключается в нахождении параметров $\theta$ и $b$, которые максимизируют функцию правдоподобия:

$$\hat{\Theta} = \arg\max_{\Theta} L(\theta, b|X, y)$$

Для упрощения вычислений обычно максимизируют логарифм правдоподобия:
$$\arg\max_{x} f(x) = \arg\max_{x} \ln f(x)$$
$$\hat{\Theta} = \arg\max_{\Theta} \ln L(\theta, b|X, y)$$

## Вывод функции потерь

Логарифм правдоподобия:

$$\ln L(\theta, b|X, y) = \sum\limits_{i=1}^n \ln\left(\frac{1}{2b} \exp\left(-\frac{|y_i - f(X_i, \theta)|}{b}\right)\right) = \sum_{i=1}^n \left(\ln\frac{1}{2b} - \frac{|y_i - f(X_i, \theta)|}{b}\right) = n \ln\frac{1}{2b} - \frac{1}{b}\sum_{i=1}^n |y_i - f(X_i, \theta)|$$

Максимизация этого выражения эквивалентна минимизации:

$$\arg\max_{\Theta}\left(n \ln\frac{1}{2b} - \frac{1}{b}\sum_{i=1}^n |y_i - f(X_i, \theta)|\right) = \arg\min_{\Theta}\left(\frac{1}{b}\sum\limits_{i=1}^n |y_i - f(X_i, \theta)| - n\ln\frac{1}{2b}\right)$$

$$\arg\min_{\Theta}\left(\frac{1}{b}\sum\limits_{i=1}^n |y_i - f(X_i, \theta)| - n\ln\frac{1}{2b}\right) = \arg\min_{\Theta}\frac{1}{b}\sum\limits_{i=1}^n |y_i - f(X_i, \theta)|$$

Заметим, что при фиксированном $b$ минимизация этой функции эквивалентна минимизации суммы абсолютных отклонений:

$$L = \sum\limits_{i=1}^n |y_i - f(X_i, \theta)| - функция~потерь$$

Это соответствует методу наименьших модулей в отличие от метода наименьших квадратов, который получается при предположении о нормальном распределении ошибок.
